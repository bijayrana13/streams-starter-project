KAFKA STREAMS (High Level DSL)
==============================

It is both a kafka consumer and producer .First library in the world to provide exactly once capabilities is kafka streams. One record at a time, no batching, true streaming..... input is kafka and output is also kafka.....so kafka to kafka


Configuration required for kafkastreams --
bootstrap.servers
auto.offset.reset.config
application.id
  Consumer group.id = application.id
  Default client.id prefix
  Prefix to internal changelog topics
default.key/value.serde

The data type of key and value are inferred during compile time.

Streams = ----
Processers = nodes


NOTE : Kafka topics should be created before starting kafka streams application.
       Kafka Stream application create two internal topics= ChangeLog(in case of key transformation) and Repartition (in case aggregation)


1)Start zookeeper and kafka
---------------------------
cd C:\Users\bijrana\Desktop\bigdata\kafka_2.11-2.2.0\bin\windows
set JAVA_HOME=C:\Program Files\Java\jre1.8.0_241
set PATH=C:\Program Files\Java\jre1.8.0_241\bin;%PATH%

c:
cd C:\Users\bijrana\Desktop\bigdata\kafka_2.11-2.2.0\bin\windows
zookeeper-server-start.bat ....\config\zookeeper.properties
kafka-server-start.bat ....\config\server.properties


2)Create below topics
---------------------
kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 2 --topic word-count-input
kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 2 --topic word-count-output



3)List the topics
-----------------
kafka-topics.sh --list --zookeeper localhost:2181

4)Start stream application

5)Read data from output topic

sh kafka-console-consumer.sh --bootstrap-server localhost:9092 \
--topic word-count-output \
--from-beginning \
--formatter kafka.tools.DefaultMessageFormatter \
--property print.key=true \
--property print.value=true \
--property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \
--property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer

6)Write data to input topic
--------------------------
kafka-console-producer.sh --broker-list localhost:9092 --topic word-count-input

7)List topic again to check changelog and repartition topics
------------------------------------------------------------
kafka-topics.sh --list --zookeeper localhost:2181

MISCS PRACTISE-->

Learn Debugging code Run as fat jar scale application



KSTREAMS :
==========

All inserts
Infinite & Unbounded data streams

use while reading data from not compacted topic
use if new data is partial/transaction info

both Map and MapValues can be used here.
Filter can be used
FlatMapValues and FlatMap can be used

E.g
KStream<String,Long> Streams = builder.stream( Serdes.String(), Serdes.Long(), "Word-Count-Input-Topic" )

KTABLE:
=======

upserts on same key and non null different values
deletes on same key and null values

use while reading data from log-compacted(aggregated) topic
use if new records are complete self sufficient and need a structured format..

only MapValues can be used here
Filter can be used
FlatMapValues and FlatMap cant be used

E.g
KTable<String,Long> Table = builder.table( Serdes.String(), Serdes.Long(), "Word-Count-Input-Topic");


NOTE: Any function with a key always triggers a re-partition.



KStream Branch --
--------------
Branch(split) a KStream based on one or more predicates.

KStream<String,String>[] branches = stream.branch(
(key, value) -> value > 100, /* first predicate /
(key, value) -> value > 10, / second predicate /
(key, value) -> value > 0, / third predicate */ )




GlobalKTable --
=============

GlobalKTable<String,Long> Streams = builder.globalTable(
Serdes.String(),
Serdes.Long(),
"Word-Count-Input-Topic");


Write kstream/ktable to a kafka topic --

stream.to("topic name")

Write KStream to topic and create a KStream out of it again --

KStream<String,Long> newStream = stream.through("topic name")
KTable<String,Long> newTable = table.through("topic name")



Note: Use functions which modifies values only(e.g FlatMapValues) if possible to avoid repartitioning.



LOG COMPACTION:
===============

Log Compaction(not for de-duplication) :
Log compaction ensures to have latest known record in kafka topic.

NOTE: Since de-duplication/log-compaction is done after segment is committed , Consumer/producer still read all data including duplicates and write them as well.
Compaction is only for new consumer/producer so that they dont read all history data of key.


tail broker.log and TRY compaction as below ==>

kafka-topics --zookeeper localhost:2181 --create \
--topic employee-salary-compact \
--partitions 1 --replication-factor 1 \
--config cleanup.policy=compact \
--config min.cleanable.dirty.ratio=0.005 \
--config segment.ms=10000

kafka-console-consumer --bootstrap-server localhost:9092 \
--topic employee-salary-compact \
--from-beginning \
--property print.key=true \
--property key.separator=,

kafka-console-producer --broker-list localhost:9092 \
--topic employee-salary-compact \
--property parse.key=true \
--property key.separator=,

Input data as below to producer

123,{"JHON":"80000"}
456,{"MARK":"90000"}
764,{"LISA":"100000"}
764,{"LISA":"110000"}
456,{"MARK":"140000"}



STream and Table Duality:
------------------------
Stream is the changelog of a table which captures the statechange of a table. Table is a snapshot of all latest value of each key

Try converting a table to stream or vice versa.


Transforming KTable to a KStream (if we need all updates/deletes/inserts)  -->

KStream<byte[], String> stream = table.toStream();


Transforming KStream to a KTable in two ways -->

1)Chain a groupByKey() and an aggregation step (count, aggregate, reduce)
KTable<String, String> table = usersAndColours.groupByKey().count();

2)Write back to kafka and read as table
stream.to("intermediate-topic");
KTable<String, String> table = builder.table("intermediate-topic");

---------------------------------------------------------------

Practise examples Favourite color App:

1)
kafka-topics.sh --delete --topic color-count-input --zookeeper localhost:2181
kafka-topics.sh --delete --topic color-count-output --zookeeper localhost:2181
kafka-topics.sh --delete --topic color-count-intermediate --zookeeper localhost:2181


2)Create below topics
kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic color-count-input
kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic color-count-output --config cleanup.policy=compact
kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic color-count-intermediate --config cleanup.policy=compact


3)List the topics bin/kafka-topics.sh --list --zookeeper localhost:2181

4)Start stream application

5)Read data from output topic

kafka-console-consumer.sh --bootstrap-server localhost:9092 \
--topic color-count-output \
--from-beginning \
--formatter kafka.tools.DefaultMessageFormatter \
--property print.key=true \
--property print.value=true \
--property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \
--property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer

kafka-console-consumer.sh --bootstrap-server localhost:9092 \
--topic color-count-intermediate \
--from-beginning \
--formatter kafka.tools.DefaultMessageFormatter \
--property print.key=true \
--property print.value=true \
--property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \
--property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer

6)Write data to input topic
kafka-console-producer.sh --broker-list localhost:9092 --topic color-count-input

-------------------------------------------------------------------------



KSTREAM and KTABLE ADVANCED STATEFULE OPERATIONS :
==================================================

KGroupedStream :
----------------
groupBy/groupByKey() on KStream
Null keys/values are ignored


KGroupedTable :
---------------
Null Keys are ignored..Null values are treated as delete.


SOME OPERATIONS -----:

AGGREGATE -->
-------------

KTable<byte[], Long> aggregatedStream = groupedStream.aggregate(
() -> 0L,
(aggKey, newValue, aggValue) -> aggValue + newValue.length(),
Serdes.Long(),
"aggregated-stream-store" );

KTable<byte[], Long> aggregatedStream = groupedStream.aggregate(
() -> 0L,
(aggKey, newValue, aggValue) -> aggValue + newValue.length(),
(aggKey, oldValue, aggValue) -> aggValue - oldValue.length(),
Serdes.Long(),
"aggregated-stream-store" );


GROUPBY -->
-----------

KGroupedTable<String, String> groupedTable = KTableData.groupBy(
(k,v) -> KeyValue.pair(v,v.length()),
Serdes.String(),
Serdes.Integer() );


REDUCE -->
----------

NOTE : same as aggragte but output type is same as input data type.

KTable<String, Long> aggregatedStream = groupedStream.reduce(
(aggValue, newValue) -> aggValue + newValue,
"reduced-stream-store" );

KTable<String, Long> aggregatedTable = groupedTable.reduce(
(aggValue, newValue) -> aggValue + newValue,
(aggValue, oldValue) -> aggValue - oldValue,
"reduced-table-store" );


PEEK -->
--------

Allows us to do side effect operation to a KStream.
To collect statistics or print some in console.

KStream<byte[], String> unmodifiedStream = stream.peek()
(key,value) -> System.out.println("key=" + key + ", value=" + value));



ENABLE EXACTLY ONCE IN KSTREAM :
--------------------------------

NOTE: Data process/commit and Offset commit happenes at time, so exactly once.
      Producers are idempotent now will keep only copy of message even if it is sent twice or more.

props.put(StreamsConfig.PROCESSING_GUANRANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);



NOTE: If any operation is not idempotent by nature then we will need exactly once there.
NOTE: Since KStream are kafka to kafka, we can use KAFKA CONNECT API if want to write data to external system.



Conversion --
==========

KStream ---> groupBy,groupByKey ---> KGroupedStream  ---> aggregate,count,reduce ---> KTable
KTable ---> groupBy ---> KGroupedTable ---> aggregate,count,reduce ---> KTable

